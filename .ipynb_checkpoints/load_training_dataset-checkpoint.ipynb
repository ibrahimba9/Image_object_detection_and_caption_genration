{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "0a3w6BiiCPJ4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pickle import dump, load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "oyEnkP1GCPJ7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "LKHEsaSBCPJ-"
   },
   "outputs": [],
   "source": [
    "def read_file(path_name):\n",
    "    f = open(path_name)\n",
    "    return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "Q4HAPRJFCPKA"
   },
   "outputs": [],
   "source": [
    "def load_training_photos():\n",
    "    image_names = read_file(\"Data/Flickr_8k.trainImages.txt\")\n",
    "    image_names.split(\"\\n\")\n",
    "    return image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "CU5fyTdCCPKC"
   },
   "outputs": [],
   "source": [
    "def get_clean_description(training_images):\n",
    "    training_desc = {}\n",
    "    descriptions = read_file(\"clean_description.txt\")\n",
    "    descriptions = descriptions.split(\"\\n\")\n",
    "    for desc in descriptions:\n",
    "        title, caption = desc.split(\"\\t\")\n",
    "        if title in training_images:\n",
    "            caption = \"<start>\" + caption + \"<end>\"\n",
    "            if title in training_desc:\n",
    "                training_desc[title].append(caption)\n",
    "            else:\n",
    "                \n",
    "                training_desc[title] = [caption]\n",
    "    return training_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "RIy-HE5cCPKE"
   },
   "outputs": [],
   "source": [
    "def load_features(training_images):\n",
    "    all_features = load(open(\"features.p\",\"rb\"))\n",
    "    features = {k:all_features[k] for k in training_images}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "moh4KElLCPKH"
   },
   "outputs": [],
   "source": [
    "training_images = load_training_photos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "rvTetuoMCPKK"
   },
   "outputs": [],
   "source": [
    "training_desc = get_clean_description(training_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "tov-l6oNCPKM",
    "outputId": "d0d4f9a6-7f63-4c06-83c5-fb6dda938c7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1000268201_693b08cb0e.jpg',\n",
       "  ['<start>a child in a pink dress is climbing up a set of stairs in an entry way <end>',\n",
       "   '<start>a girl going into a wooden building <end>',\n",
       "   '<start>a little girl climbing into a wooden playhouse <end>',\n",
       "   '<start>a little girl climbing the stairs to her playhouse <end>',\n",
       "   '<start>a little girl in a pink dress going into a wooden cabin <end>']),\n",
       " ('1001773457_577c3a7d70.jpg',\n",
       "  ['<start>a black dog and a spotted dog are fighting<end>',\n",
       "   '<start>a black dog and a tri colored dog playing with each other on the road <end>',\n",
       "   '<start>a black dog and a white dog with brown spots are staring at each other in the street <end>',\n",
       "   '<start>two dogs of different breeds looking at each other on the road <end>',\n",
       "   '<start>two dogs on pavement moving toward each other <end>']),\n",
       " ('1002674143_1b742ab4b8.jpg',\n",
       "  ['<start>a little girl covered in paint sits in front of a painted rainbow with her hands in a bowl <end>',\n",
       "   '<start>a little girl is sitting in front of a large painted rainbow <end>',\n",
       "   '<start>a small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it <end>',\n",
       "   '<start>there is a girl with pigtails sitting in front of a rainbow painting <end>',\n",
       "   '<start>young girl with pigtails painting outside in the grass <end>'])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_desc.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "2luNc_SrCPKP"
   },
   "outputs": [],
   "source": [
    "features = load_features(training_images.split(\"\\n\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Mzi5txzjCPKR",
    "outputId": "ab5f23c2-7580-40e9-a52d-a11b1145a476"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2513260012_03d33305cf.jpg',\n",
       "  array([[0.        , 0.39944476, 0.00230728, ..., 0.24724662, 0.00611375,\n",
       "          0.01884155]], dtype=float32)),\n",
       " ('2903617548_d3e38d7f88.jpg',\n",
       "  array([[0.14825103, 0.0083261 , 0.10619269, ..., 0.09697448, 0.24103813,\n",
       "          0.031633  ]], dtype=float32)),\n",
       " ('3338291921_fe7ae0c8f8.jpg',\n",
       "  array([[0.3214843 , 0.        , 0.00088847, ..., 0.        , 0.04906381,\n",
       "          0.        ]], dtype=float32))]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n80Zr3lCPKT"
   },
   "source": [
    "### extract training vector features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "vUFlF2GhCPKT"
   },
   "outputs": [],
   "source": [
    "def get_training_features(training_images, features):\n",
    "    training_features = {}\n",
    "    for img, vector in features.items():\n",
    "        if img in training_images:\n",
    "            training_features[img] = vector\n",
    "    return training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "mMDupstaCPKV"
   },
   "outputs": [],
   "source": [
    "training_features = get_training_features(training_images, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "Mk568xdeCPKX",
    "outputId": "128e566a-f948-4c7f-e44e-ab32723f0997"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLEBM4dnCPKZ"
   },
   "source": [
    "### Tokenizing the vocabulary of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "bcR7W_2GCPKZ"
   },
   "outputs": [],
   "source": [
    "list_desc = []\n",
    "for img in training_desc:\n",
    "    [list_desc.append(d) for d in training_desc[img]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "itPK6Yb8CPKb"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "HZ1wAlwJCPKd"
   },
   "outputs": [],
   "source": [
    "dump(tokenizer, open('tokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "SQ8tyx6gCPKf",
    "outputId": "3374b1b9-bda2-49c9-e61d-dcb52b61cde8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7333"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "3pLKVbXgCPKh",
    "outputId": "f39f32c0-e269-4db1-83f4-8d62134b20f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7334"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNG8iPTHCPKj"
   },
   "source": [
    "### Maximum length of a description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "0iYPcLDlCPKj"
   },
   "outputs": [],
   "source": [
    "max_length = max(len(d.split()) for d in list_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTL1cRPICPKl"
   },
   "source": [
    "### number of training instances (images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "ooK-IgLSCPKl",
    "outputId": "e842299d-e049-42db-fafe-a66c93f0270e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_desc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "zV38817jCPKn"
   },
   "outputs": [],
   "source": [
    "feat_file = open('features.p', 'rb')\n",
    "\n",
    "features = load(feat_file)\n",
    "\n",
    "feat_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EaYfvd1CPKp"
   },
   "source": [
    "### Every training image will have 2048 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "7mR6DRsjCPKp",
    "outputId": "a8317853-e47b-4c59-8f33-0c96be2a4399"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2048)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.items())[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBNeQHdsCPKr"
   },
   "source": [
    "### 2048 features in addition of the number of tokens from the description (max = 32) is impossible to handle into memory while training 6000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "iFW2OeTxCPKs"
   },
   "outputs": [],
   "source": [
    "def create_data_sequences(tokenizer, max_length, img_desc, features):\n",
    "    \"\"\"\n",
    "        for each input instances, this function will create a sequence of description tokens as an input and the next token as an output\n",
    "        X1: feature vector of the input image\n",
    "        X2: sequence of input tokens of each description\n",
    "        Y: the next token of the input sequence\n",
    "    \"\"\"\n",
    "    X1, X2, Y = [], [], []\n",
    "    for desc in img_desc:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            input_seq, output_seq = seq[:i], seq[i]\n",
    "            input_seq = pad_sequences([input_seq], max_length)[0] #to make sure the input has len=32 for all sequnces of descriptions\n",
    "            output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(features)\n",
    "            X2.append(input_seq)\n",
    "            Y.append(output_seq)\n",
    "    return np.array(X1).reshape(np.array(X1).shape[0], np.array(X1).shape[2]), np.array(X2), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7umd96FCPKu"
   },
   "source": [
    "### Since w have a lot of data we will use a generator to create the input features and the input sequences for each instance\n",
    "### The generator will only be executed once and the yield function will return the generated data one by one\n",
    "### this method will help us save memory since we don't need to store the data, they will only be used while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "Ouho9DrRCPKu"
   },
   "outputs": [],
   "source": [
    "def generate_data(training_desc, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, img_descs in training_desc.items():\n",
    "            img_feature_vector = features[key]\n",
    "            input_image, input_sequence, output = create_data_sequences(tokenizer, max_length, img_descs, img_feature_vector)\n",
    "            yield ([input_image, input_sequence], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "jpUKhTOJCPKw"
   },
   "outputs": [],
   "source": [
    "[a,b],c = next(generate_data(training_desc, features, tokenizer, max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p334sqlOCPKy"
   },
   "source": [
    "### a represents the input matrix of the vector feature\n",
    "- 2048 is length on the vector feature (the output shape of the Xception architecture we used earlier to extract features)\n",
    "- 58 the number of sequences (tokens of the description) generated for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "EWyXSAbPCPKz",
    "outputId": "1e659c20-56ce-4505-acd8-69eaa363bafc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 2048)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERWJX3pACPK1"
   },
   "source": [
    "### b is the matrix that contains the input sequences of the description of an instance\n",
    "- 58 number of sequences generated\n",
    "- 37 length of the sequence (max length of a description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "92Z6a-Q1CPK1",
    "outputId": "e45cf9a6-c068-48c4-88f2-c15838fa73a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 37)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LWSNryvCPK4"
   },
   "source": [
    "### c is the output matrix which represents the next token of the input sequence\n",
    "- 58 the number of sequences of an input instance, so we need the next token for each sequence\n",
    "- 7334 the number of existing unique words in the vocabulary of the description (training intances), we converted the vocabulary to a binary representation (vector of 0s and 1) since the computer doesn't understand englidh words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "Rt-Wko_BCPK4",
    "outputId": "ed681298-e455-43e8-8948-df01b842c550"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 7334)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVY4uMO_CPK7"
   },
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "Rq2IDp2sCPK7"
   },
   "outputs": [],
   "source": [
    "input_features = Input(shape=(2048,))\n",
    "regul_drop = Dropout(0.5)(input_features)\n",
    "dense1 = Dense(256, activation=\"relu\")(regul_drop)\n",
    "\n",
    "input_seq = Input(shape=(max_length,))\n",
    "layer1 = Embedding(vocab_size, 256, mask_zero=True)(input_seq)\n",
    "regul_drop_seq = Dropout(0.5)(layer1)\n",
    "lstm_layer =LSTM(256)(regul_drop_seq)\n",
    "\n",
    "decoder1 = add([dense1, lstm_layer])\n",
    "\n",
    "decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "output = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "model = Model(inputs=[input_features, input_seq], outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "ClHYjuZcCPK8",
    "outputId": "b093954d-99c4-41ec-ec4a-99734d275b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 37)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 37, 256)      1877504     input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 2048)         0           input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 37, 256)      0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 256)          524544      dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 256)          525312      dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 256)          0           dense_27[0][0]                   \n",
      "                                                                 lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 256)          65792       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 7334)         1884838     dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,877,990\n",
      "Trainable params: 4,877,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOorXekSCPK-"
   },
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HidOSd8cCPK_"
   },
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kxlAYeSCPK_",
    "outputId": "86bd7242-737b-4e9d-cfa3-7485befb1662"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps = len(training_desc)\n",
    "\n",
    "for i in range(3):\n",
    "    generator = generate_data(training_desc, training_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"models/model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiQd1k-PCPLG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "load_training_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

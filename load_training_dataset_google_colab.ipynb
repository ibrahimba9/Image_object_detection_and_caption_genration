{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook shows how to clone a github repository to google drive and run the notebook on google colab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount your google drive to colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G60dDK2YJ22h",
    "outputId": "1ea2466f-8834-4f5d-8efc-a3bed559ad6a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a folder in your drive\n",
    "### Redirect to the path of the folder created to clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9x5fGtzKBuz",
    "outputId": "bf9cdbb2-b519-48ca-e570-3408f990d4ae"
   },
   "outputs": [],
   "source": [
    "%cd 'gdrive/My Drive/caption_generation_clone/Image_object_detection_and_caption_genration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1LTrcVcHJzES"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pickle import dump, load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmdF4MpcLpjt",
    "outputId": "21043f71-a606-4352-b492-5b9ce8db4bad"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owhZE6lVLzII",
    "outputId": "1f4ad761-4169-4c4d-afad-0e0187730fb3"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOkz70x5MH7U",
    "outputId": "e9610577-1270-4d42-99c4-7c65e07effc2"
   },
   "outputs": [],
   "source": [
    "!unzip \\*.zip  && rm *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J7jtaJYWJzEX"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OWk2g8MmJzEZ"
   },
   "outputs": [],
   "source": [
    "def read_file(path_name):\n",
    "    f = open(path_name)\n",
    "    return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LWS3bndMJzEb"
   },
   "outputs": [],
   "source": [
    "def load_training_photos():\n",
    "    image_names = read_file(\"Flickr_8k.trainImages.txt\")\n",
    "    image_names.split(\"\\n\")\n",
    "    return image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jsXWQNpGJzEd"
   },
   "outputs": [],
   "source": [
    "def get_clean_description(training_images):\n",
    "    training_desc = {}\n",
    "    descriptions = read_file(\"clean_description.txt\")\n",
    "    descriptions = descriptions.split(\"\\n\")\n",
    "    for desc in descriptions:\n",
    "        title, caption = desc.split(\"\\t\")\n",
    "        if title in training_images:\n",
    "            caption = \"<start>\" + caption + \"<end>\"\n",
    "            if title in training_desc:\n",
    "                training_desc[title].append(caption)\n",
    "            else:\n",
    "                \n",
    "                training_desc[title] = [caption]\n",
    "    return training_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6H_7gpiEJzEf"
   },
   "outputs": [],
   "source": [
    "def load_features(training_images):\n",
    "    all_features = load(open(\"features.p\",\"rb\"))\n",
    "    features = {k:all_features[k] for k in training_images}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mEl4nKlEJzEh"
   },
   "outputs": [],
   "source": [
    "training_images = load_training_photos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-eqPRKkyJzEk"
   },
   "outputs": [],
   "source": [
    "training_desc = get_clean_description(training_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMbX5SSwJzEm",
    "outputId": "64a626ba-ca74-4d30-887f-68233050f76b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1000268201_693b08cb0e.jpg',\n",
       "  ['<start>a child in a pink dress is climbing up a set of stairs in an entry way <end>',\n",
       "   '<start>a girl going into a wooden building <end>',\n",
       "   '<start>a little girl climbing into a wooden playhouse <end>',\n",
       "   '<start>a little girl climbing the stairs to her playhouse <end>',\n",
       "   '<start>a little girl in a pink dress going into a wooden cabin <end>']),\n",
       " ('1001773457_577c3a7d70.jpg',\n",
       "  ['<start>a black dog and a spotted dog are fighting<end>',\n",
       "   '<start>a black dog and a tri colored dog playing with each other on the road <end>',\n",
       "   '<start>a black dog and a white dog with brown spots are staring at each other in the street <end>',\n",
       "   '<start>two dogs of different breeds looking at each other on the road <end>',\n",
       "   '<start>two dogs on pavement moving toward each other <end>']),\n",
       " ('1002674143_1b742ab4b8.jpg',\n",
       "  ['<start>a little girl covered in paint sits in front of a painted rainbow with her hands in a bowl <end>',\n",
       "   '<start>a little girl is sitting in front of a large painted rainbow <end>',\n",
       "   '<start>a small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it <end>',\n",
       "   '<start>there is a girl with pigtails sitting in front of a rainbow painting <end>',\n",
       "   '<start>young girl with pigtails painting outside in the grass <end>'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_desc.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6WGBwCy0JzEo"
   },
   "outputs": [],
   "source": [
    "features = load_features(training_images.split(\"\\n\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75KnfgVTJzEq",
    "outputId": "00c4b3df-308c-409a-f4bb-6f1e80a38c13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2513260012_03d33305cf.jpg',\n",
       "  array([[0.        , 0.39944476, 0.00230728, ..., 0.24724662, 0.00611375,\n",
       "          0.01884155]], dtype=float32)),\n",
       " ('2903617548_d3e38d7f88.jpg',\n",
       "  array([[0.14825103, 0.0083261 , 0.10619269, ..., 0.09697448, 0.24103813,\n",
       "          0.031633  ]], dtype=float32)),\n",
       " ('3338291921_fe7ae0c8f8.jpg',\n",
       "  array([[0.3214843 , 0.        , 0.00088847, ..., 0.        , 0.04906381,\n",
       "          0.        ]], dtype=float32))]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28VVGN0FJzEs"
   },
   "source": [
    "### extract training vector features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "c6LOKVaTJzEs"
   },
   "outputs": [],
   "source": [
    "def get_training_features(training_images, features):\n",
    "    training_features = {}\n",
    "    for img, vector in features.items():\n",
    "        if img in training_images:\n",
    "            training_features[img] = vector\n",
    "    return training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "KG26CD1XJzEu"
   },
   "outputs": [],
   "source": [
    "training_features = get_training_features(training_images, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmNI-iPCJzEw",
    "outputId": "a1e2055c-af8c-4aaf-bd5a-7d548739a464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDzMU2orJzEy"
   },
   "source": [
    "### Tokenizing the vocabulary of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nkW2za0mJzEz"
   },
   "outputs": [],
   "source": [
    "list_desc = []\n",
    "for img in training_desc:\n",
    "    [list_desc.append(d) for d in training_desc[img]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Z1nymn17JzE0"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4GuC9YUdJzE4"
   },
   "outputs": [],
   "source": [
    "dump(tokenizer, open('tokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEh-p8eiJzE5",
    "outputId": "3df33bf9-8d8a-49f1-fa70-a80aa008efb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7333"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPN7VAsuJzE7",
    "outputId": "66e153fc-b3ad-4906-8a53-bb007bcf4199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7334"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW6eZC9wJzE9"
   },
   "source": [
    "### Maximum length of a description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MhgH1BU-JzE9"
   },
   "outputs": [],
   "source": [
    "max_length = max(len(d.split()) for d in list_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf_S-WRMJzE_"
   },
   "source": [
    "### number of training instances (images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNUw8Av1JzE_",
    "outputId": "dde45f6a-e3e8-4bae-d13a-d861cf067520"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_desc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "iGlaIvd8JzFB"
   },
   "outputs": [],
   "source": [
    "feat_file = open('features.p', 'rb')\n",
    "\n",
    "features = load(feat_file)\n",
    "\n",
    "feat_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgXI9-lRJzFD"
   },
   "source": [
    "### Every training image will have 2048 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCa-i_fRJzFD",
    "outputId": "e768784a-3d9e-463e-d509-14a7ddff75d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2048)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.items())[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h1A8ylRJzFF"
   },
   "source": [
    "### 2048 features in addition of the number of tokens from the description (max = 32) is impossible to handle into memory while training 6000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Z2mRApdMJzFG"
   },
   "outputs": [],
   "source": [
    "def create_data_sequences(tokenizer, max_length, img_desc, features):\n",
    "    \"\"\"\n",
    "        for each input instances, this function will create a sequence of description tokens as an input and the next token as an output\n",
    "        X1: feature vector of the input image\n",
    "        X2: sequence of input tokens of each description\n",
    "        Y: the next token of the input sequence\n",
    "    \"\"\"\n",
    "    X1, X2, Y = [], [], []\n",
    "    for desc in img_desc:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            input_seq, output_seq = seq[:i], seq[i]\n",
    "            input_seq = pad_sequences([input_seq], max_length)[0] #to make sure the input has len=32 for all sequnces of descriptions\n",
    "            output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(features)\n",
    "            X2.append(input_seq)\n",
    "            Y.append(output_seq)\n",
    "    return np.array(X1).reshape(np.array(X1).shape[0], np.array(X1).shape[2]), np.array(X2), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmrxXNiJJzFH"
   },
   "source": [
    "### Since w have a lot of data we will use a generator to create the input features and the input sequences for each instance\n",
    "### The generator will only be executed once and the yield function will return the generated data one by one\n",
    "### this method will help us save memory since we don't need to store the data, they will only be used while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "UXWyGYOUJzFI"
   },
   "outputs": [],
   "source": [
    "def generate_data(training_desc, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, img_descs in training_desc.items():\n",
    "            img_feature_vector = features[key]\n",
    "            input_image, input_sequence, output = create_data_sequences(tokenizer, max_length, img_descs, img_feature_vector)\n",
    "            yield ([input_image, input_sequence], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "vPzo_aelJzFJ"
   },
   "outputs": [],
   "source": [
    "[a,b],c = next(generate_data(training_desc, features, tokenizer, max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74XXC951JzFM"
   },
   "source": [
    "### a represents the input matrix of the vector feature\n",
    "- 2048 is length on the vector feature (the output shape of the Xception architecture we used earlier to extract features)\n",
    "- 58 the number of sequences (tokens of the description) generated for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBeNP5u9JzFM",
    "outputId": "cd2b8707-7b8b-46a1-dc04-5dbac83b9f76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 2048)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuk8Cbl4JzFN"
   },
   "source": [
    "### b is the matrix that contains the input sequences of the description of an instance\n",
    "- 58 number of sequences generated\n",
    "- 37 length of the sequence (max length of a description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lz42tynpJzFO",
    "outputId": "b4a7bd78-874b-46a0-bb59-cb38677c1bb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 37)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fepCF8FXJzFQ"
   },
   "source": [
    "### c is the output matrix which represents the next token of the input sequence\n",
    "- 58 the number of sequences of an input instance, so we need the next token for each sequence\n",
    "- 7334 the number of existing unique words in the vocabulary of the description (training intances), we converted the vocabulary to a binary representation (vector of 0s and 1) since the computer doesn't understand englidh words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXLMYuWnJzFQ",
    "outputId": "0f9b8883-f189-4b55-e661-fbf2909ce1f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 7334)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL_USWyfJzFS"
   },
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YAJshy7-JzFS"
   },
   "outputs": [],
   "source": [
    "input_features = Input(shape=(2048,))\n",
    "regul_drop = Dropout(0.5)(input_features)\n",
    "dense1 = Dense(256, activation=\"relu\")(regul_drop)\n",
    "\n",
    "input_seq = Input(shape=(max_length,))\n",
    "layer1 = Embedding(vocab_size, 256, mask_zero=True)(input_seq)\n",
    "regul_drop_seq = Dropout(0.5)(layer1)\n",
    "lstm_layer =LSTM(256)(regul_drop_seq)\n",
    "\n",
    "decoder1 = add([dense1, lstm_layer])\n",
    "\n",
    "decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "output = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "model = Model(inputs=[input_features, input_seq], outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "no3pta4iJzFU",
    "outputId": "7adf8e5e-a2eb-43d4-eb23-841728b3fc4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 37)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 37, 256)      1877504     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 37, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7334)         1884838     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,877,990\n",
      "Trainable params: 4,877,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuYwfxQpJzFW"
   },
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2Vd3rCgJzFW"
   },
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3XpgVmwmJzFW",
    "outputId": "6913b422-da1a-48cc-c2fa-7cfdc8abd832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 680s 113ms/step - loss: 3.1727\n",
      "6000/6000 [==============================] - 672s 112ms/step - loss: 2.9312\n",
      "6000/6000 [==============================] - 670s 112ms/step - loss: 2.7806\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "steps = len(training_desc)\n",
    "\n",
    "for i in range(3):\n",
    "    generator = generate_data(training_desc, training_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"models/model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcOY-ocjJzFc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "load_training_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
